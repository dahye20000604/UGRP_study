{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Uijeong'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(r\"C:\\Users\\Uijeong\\Desktop\\05-11--machine-learning\\Part 05~11) Machine Learning\\08. Ensemble 기법의 종류와 원리\\Data\\otto_train.csv\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의미가 없다고 판단되는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:57:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:57:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 5.28 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "#objective 와 eval_metrix 등의 하이퍼 파라미터 잘 구분하기\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uijeong\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 3.34 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 653ms\tremaining: 1m 4s\n",
      "1:\tlearn: 0.6356107\ttotal: 1.18s\tremaining: 57.9s\n",
      "2:\tlearn: 0.6411256\ttotal: 1.71s\tremaining: 55.3s\n",
      "3:\tlearn: 0.6480344\ttotal: 2.27s\tremaining: 54.5s\n",
      "4:\tlearn: 0.6508222\ttotal: 2.81s\tremaining: 53.5s\n",
      "5:\tlearn: 0.6499939\ttotal: 3.37s\tremaining: 52.8s\n",
      "6:\tlearn: 0.6507818\ttotal: 3.96s\tremaining: 52.6s\n",
      "7:\tlearn: 0.6548422\ttotal: 4.54s\tremaining: 52.2s\n",
      "8:\tlearn: 0.6559533\ttotal: 5.12s\tremaining: 51.8s\n",
      "9:\tlearn: 0.6560947\ttotal: 5.71s\tremaining: 51.4s\n",
      "10:\tlearn: 0.6568421\ttotal: 6.29s\tremaining: 50.9s\n",
      "11:\tlearn: 0.6588219\ttotal: 6.83s\tremaining: 50.1s\n",
      "12:\tlearn: 0.6592259\ttotal: 7.36s\tremaining: 49.2s\n",
      "13:\tlearn: 0.6611248\ttotal: 7.89s\tremaining: 48.5s\n",
      "14:\tlearn: 0.6625591\ttotal: 8.44s\tremaining: 47.8s\n",
      "15:\tlearn: 0.6631853\ttotal: 9s\tremaining: 47.2s\n",
      "16:\tlearn: 0.6639328\ttotal: 9.54s\tremaining: 46.6s\n",
      "17:\tlearn: 0.6668821\ttotal: 10.1s\tremaining: 46s\n",
      "18:\tlearn: 0.6669630\ttotal: 10.6s\tremaining: 45.4s\n",
      "19:\tlearn: 0.6675286\ttotal: 11.2s\tremaining: 44.7s\n",
      "20:\tlearn: 0.6673266\ttotal: 11.7s\tremaining: 44.1s\n",
      "21:\tlearn: 0.6677104\ttotal: 12.3s\tremaining: 43.5s\n",
      "22:\tlearn: 0.6682558\ttotal: 12.8s\tremaining: 42.9s\n",
      "23:\tlearn: 0.6683972\ttotal: 13.4s\tremaining: 42.3s\n",
      "24:\tlearn: 0.6686599\ttotal: 14s\tremaining: 41.9s\n",
      "25:\tlearn: 0.6681952\ttotal: 14.5s\tremaining: 41.4s\n",
      "26:\tlearn: 0.6684982\ttotal: 15.2s\tremaining: 41s\n",
      "27:\tlearn: 0.6692053\ttotal: 15.7s\tremaining: 40.5s\n",
      "28:\tlearn: 0.6696699\ttotal: 16.4s\tremaining: 40s\n",
      "29:\tlearn: 0.6699325\ttotal: 16.9s\tremaining: 39.5s\n",
      "30:\tlearn: 0.6705992\ttotal: 17.5s\tremaining: 39s\n",
      "31:\tlearn: 0.6709426\ttotal: 18.1s\tremaining: 38.5s\n",
      "32:\tlearn: 0.6708012\ttotal: 18.7s\tremaining: 38s\n",
      "33:\tlearn: 0.6709426\ttotal: 19.3s\tremaining: 37.4s\n",
      "34:\tlearn: 0.6707002\ttotal: 19.8s\tremaining: 36.9s\n",
      "35:\tlearn: 0.6715082\ttotal: 20.4s\tremaining: 36.3s\n",
      "36:\tlearn: 0.6705992\ttotal: 21s\tremaining: 35.7s\n",
      "37:\tlearn: 0.6725991\ttotal: 21.5s\tremaining: 35.2s\n",
      "38:\tlearn: 0.6729829\ttotal: 22.2s\tremaining: 34.7s\n",
      "39:\tlearn: 0.6725991\ttotal: 22.8s\tremaining: 34.2s\n",
      "40:\tlearn: 0.6734273\ttotal: 23.4s\tremaining: 33.6s\n",
      "41:\tlearn: 0.6738314\ttotal: 24s\tremaining: 33.1s\n",
      "42:\tlearn: 0.6741546\ttotal: 24.5s\tremaining: 32.5s\n",
      "43:\tlearn: 0.6739728\ttotal: 25.1s\tremaining: 32s\n",
      "44:\tlearn: 0.6741950\ttotal: 25.7s\tremaining: 31.4s\n",
      "45:\tlearn: 0.6750636\ttotal: 26.3s\tremaining: 30.9s\n",
      "46:\tlearn: 0.6758919\ttotal: 26.9s\tremaining: 30.3s\n",
      "47:\tlearn: 0.6757707\ttotal: 27.5s\tremaining: 29.7s\n",
      "48:\tlearn: 0.6762151\ttotal: 28s\tremaining: 29.2s\n",
      "49:\tlearn: 0.6774474\ttotal: 28.6s\tremaining: 28.6s\n",
      "50:\tlearn: 0.6777100\ttotal: 29.2s\tremaining: 28.1s\n",
      "51:\tlearn: 0.6786594\ttotal: 29.8s\tremaining: 27.5s\n",
      "52:\tlearn: 0.6789827\ttotal: 30.4s\tremaining: 27s\n",
      "53:\tlearn: 0.6804372\ttotal: 31s\tremaining: 26.4s\n",
      "54:\tlearn: 0.6804372\ttotal: 31.6s\tremaining: 25.8s\n",
      "55:\tlearn: 0.6809220\ttotal: 32.2s\tremaining: 25.3s\n",
      "56:\tlearn: 0.6812250\ttotal: 32.7s\tremaining: 24.7s\n",
      "57:\tlearn: 0.6813058\ttotal: 33.3s\tremaining: 24.1s\n",
      "58:\tlearn: 0.6811846\ttotal: 33.9s\tremaining: 23.6s\n",
      "59:\tlearn: 0.6813260\ttotal: 34.5s\tremaining: 23s\n",
      "60:\tlearn: 0.6816694\ttotal: 35.1s\tremaining: 22.4s\n",
      "61:\tlearn: 0.6823159\ttotal: 35.7s\tremaining: 21.9s\n",
      "62:\tlearn: 0.6832653\ttotal: 36.3s\tremaining: 21.3s\n",
      "63:\tlearn: 0.6840734\ttotal: 36.9s\tremaining: 20.7s\n",
      "64:\tlearn: 0.6840734\ttotal: 37.4s\tremaining: 20.2s\n",
      "65:\tlearn: 0.6846592\ttotal: 38s\tremaining: 19.6s\n",
      "66:\tlearn: 0.6843360\ttotal: 38.6s\tremaining: 19s\n",
      "67:\tlearn: 0.6846390\ttotal: 39.2s\tremaining: 18.4s\n",
      "68:\tlearn: 0.6854269\ttotal: 39.8s\tremaining: 17.9s\n",
      "69:\tlearn: 0.6858309\ttotal: 40.4s\tremaining: 17.3s\n",
      "70:\tlearn: 0.6858309\ttotal: 40.9s\tremaining: 16.7s\n",
      "71:\tlearn: 0.6865783\ttotal: 41.5s\tremaining: 16.2s\n",
      "72:\tlearn: 0.6864167\ttotal: 42.1s\tremaining: 15.6s\n",
      "73:\tlearn: 0.6868611\ttotal: 42.7s\tremaining: 15s\n",
      "74:\tlearn: 0.6869217\ttotal: 43.3s\tremaining: 14.4s\n",
      "75:\tlearn: 0.6870429\ttotal: 43.9s\tremaining: 13.9s\n",
      "76:\tlearn: 0.6875278\ttotal: 44.5s\tremaining: 13.3s\n",
      "77:\tlearn: 0.6881136\ttotal: 45.1s\tremaining: 12.7s\n",
      "78:\tlearn: 0.6883762\ttotal: 45.7s\tremaining: 12.1s\n",
      "79:\tlearn: 0.6888207\ttotal: 46.3s\tremaining: 11.6s\n",
      "80:\tlearn: 0.6892449\ttotal: 46.8s\tremaining: 11s\n",
      "81:\tlearn: 0.6898509\ttotal: 47.4s\tremaining: 10.4s\n",
      "82:\tlearn: 0.6897095\ttotal: 48s\tremaining: 9.83s\n",
      "83:\tlearn: 0.6902549\ttotal: 48.6s\tremaining: 9.26s\n",
      "84:\tlearn: 0.6909822\ttotal: 49.2s\tremaining: 8.68s\n",
      "85:\tlearn: 0.6910832\ttotal: 49.8s\tremaining: 8.11s\n",
      "86:\tlearn: 0.6914468\ttotal: 50.4s\tremaining: 7.53s\n",
      "87:\tlearn: 0.6916084\ttotal: 51s\tremaining: 6.95s\n",
      "88:\tlearn: 0.6919922\ttotal: 51.6s\tremaining: 6.37s\n",
      "89:\tlearn: 0.6925579\ttotal: 52.2s\tremaining: 5.8s\n",
      "90:\tlearn: 0.6928407\ttotal: 52.7s\tremaining: 5.22s\n",
      "91:\tlearn: 0.6930427\ttotal: 53.3s\tremaining: 4.64s\n",
      "92:\tlearn: 0.6935073\ttotal: 53.9s\tremaining: 4.06s\n",
      "93:\tlearn: 0.6940932\ttotal: 54.5s\tremaining: 3.48s\n",
      "94:\tlearn: 0.6944972\ttotal: 55.2s\tremaining: 2.9s\n",
      "95:\tlearn: 0.6948810\ttotal: 55.8s\tremaining: 2.32s\n",
      "96:\tlearn: 0.6951840\ttotal: 56.4s\tremaining: 1.74s\n",
      "97:\tlearn: 0.6954264\ttotal: 57s\tremaining: 1.16s\n",
      "98:\tlearn: 0.6955881\ttotal: 57.6s\tremaining: 582ms\n",
      "99:\tlearn: 0.6956285\ttotal: 58.2s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 58.52 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(r\"C:\\Users\\Uijeong\\Desktop\\05-11--machine-learning\\Part 05~11) Machine Learning\\08. Ensemble 기법의 종류와 원리\\Data\\kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uijeong\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9567\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540425.102584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uijeong\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9603\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538335.442924\n",
      "9610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539580.189173\n",
      "9522\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533878.646308\n",
      "9517\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538634.669377\n",
      "9609\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542007.104171\n",
      "9590\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535176.917377\n",
      "9525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534451.143235\n",
      "9654\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536226.413775\n",
      "9510\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535794.908322\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(10):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index,]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 14, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([482330.63554194, 654615.5001388 , 888761.401007  , ...,\n",
       "        330942.88796679, 805859.0119842 , 458443.65295364]),\n",
       " array([ 510066.30883171,  596828.54290438, 1000103.17836497, ...,\n",
       "         344136.26847138,  926467.42017967,  465144.32065118]),\n",
       " array([513657.42104999, 584904.18265934, 933030.54950658, ...,\n",
       "        352683.09387307, 887568.20807252, 472863.73498522]),\n",
       " array([539925.97770557, 655680.42630882, 921824.81863362, ...,\n",
       "        333230.856273  , 916758.73439247, 458535.46957526]),\n",
       " array([ 525802.93675858,  622305.20393426,  966497.42852527, ...,\n",
       "         348793.92210557, 1006946.61771896,  462193.36828417]),\n",
       " array([512033.08138783, 603065.17737638, 875509.68831319, ...,\n",
       "        332731.6633402 , 935817.08807834, 473762.7107168 ]),\n",
       " array([521657.20643865, 658005.22340683, 910372.74355285, ...,\n",
       "        340057.50893114, 903860.38112518, 468389.0100951 ]),\n",
       " array([515909.92081191, 596375.57971297, 968447.22146594, ...,\n",
       "        342591.30666744, 923408.55773325, 464998.92054209]),\n",
       " array([496432.01884762, 613012.68520442, 954041.368046  , ...,\n",
       "        339969.14500099, 890316.76721937, 442737.19221462]),\n",
       " array([513229.06579719, 664084.85579111, 965890.39745903, ...,\n",
       "        336201.56385946, 874040.34853211, 459326.05980264])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 210176.6873959382\n"
     ]
    }
   ],
   "source": [
    "# 예측한 결과값들의 평균을 계산하여 실제 테스트 데이트의 타겟변수와 비교하여 성능 평가\n",
    "\n",
    "print(\"RMSE: {}\".format(sqrt(mean_squared_error(bagging_predict, test_y)))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[513104.4573170985,\n",
       " 624887.73774373,\n",
       " 938447.8794874453,\n",
       " 1593328.9928388793,\n",
       " 635886.4027799169,\n",
       " 371582.99332303426,\n",
       " 707988.8397462799,\n",
       " 428131.1829623588,\n",
       " 461311.2993771308,\n",
       " 498470.9826381691,\n",
       " 642870.5688733633,\n",
       " 392115.58664632833,\n",
       " 299978.17566669226,\n",
       " 355534.299024077,\n",
       " 339833.25901467865,\n",
       " 1274311.3789496846,\n",
       " 375803.50987198495,\n",
       " 1032677.0221798343,\n",
       " 320198.3556909847,\n",
       " 531167.9948386799,\n",
       " 381953.74057629897,\n",
       " 1974678.0705569251,\n",
       " 665386.0619279689,\n",
       " 543549.3964496205,\n",
       " 511099.3345524521,\n",
       " 481707.47246914887,\n",
       " 298056.17684567487,\n",
       " 254759.37599032922,\n",
       " 473518.3129589975,\n",
       " 529440.8827104678,\n",
       " 493435.42825517606,\n",
       " 468979.2212445817,\n",
       " 465677.29231934744,\n",
       " 576985.801594104,\n",
       " 378254.9256842045,\n",
       " 1022023.6511110144,\n",
       " 901157.9735119122,\n",
       " 525710.4718921247,\n",
       " 357439.52565160894,\n",
       " 1615257.149678586,\n",
       " 390961.2382308327,\n",
       " 273924.35753958556,\n",
       " 508799.9667717423,\n",
       " 342601.9658113391,\n",
       " 256697.55104182917,\n",
       " 247287.73055201693,\n",
       " 328830.0297541275,\n",
       " 335111.5532865908,\n",
       " 356183.37518297235,\n",
       " 568327.3203096886,\n",
       " 370746.29634164984,\n",
       " 341997.1022028016,\n",
       " 774769.7215705954,\n",
       " 334564.41149173165,\n",
       " 463757.95724343666,\n",
       " 1813687.0471489937,\n",
       " 476663.69631720753,\n",
       " 713351.2340732815,\n",
       " 331450.99776728975,\n",
       " 654590.5729410781,\n",
       " 478102.82408717426,\n",
       " 373348.1622445744,\n",
       " 297707.5386090811,\n",
       " 525472.4983326816,\n",
       " 462114.3269958145,\n",
       " 280103.5491163618,\n",
       " 389131.4837209669,\n",
       " 1519825.6046659288,\n",
       " 487385.7528715705,\n",
       " 663883.3839898735,\n",
       " 433475.40229791484,\n",
       " 300289.9241379683,\n",
       " 765206.4475558266,\n",
       " 532575.027892478,\n",
       " 509656.7934202767,\n",
       " 1243731.847583241,\n",
       " 807600.7771843025,\n",
       " 286123.569304221,\n",
       " 453915.066137477,\n",
       " 908748.9268699076,\n",
       " 635652.006402273,\n",
       " 373763.74243835674,\n",
       " 656203.8974081764,\n",
       " 356604.36297875346,\n",
       " 827789.457055088,\n",
       " 523516.16448421555,\n",
       " 513146.4188531923,\n",
       " 555608.3292956429,\n",
       " 354992.7196242445,\n",
       " 469976.02767796424,\n",
       " 350039.6105625627,\n",
       " 401514.9292090706,\n",
       " 632046.69307157,\n",
       " 1051401.6700096128,\n",
       " 426574.4491621695,\n",
       " 496722.7883541286,\n",
       " 356684.3676063906,\n",
       " 310032.95602262736,\n",
       " 817041.5280132676,\n",
       " 457178.345706565,\n",
       " 258649.07117338665,\n",
       " 912921.705464835,\n",
       " 976233.5668989051,\n",
       " 478368.48285044974,\n",
       " 1048724.2748109656,\n",
       " 299153.38132849586,\n",
       " 494975.73450018617,\n",
       " 482166.1005631808,\n",
       " 808635.9976612233,\n",
       " 2287699.985488312,\n",
       " 548386.9986428899,\n",
       " 323612.228531258,\n",
       " 558137.125293971,\n",
       " 627209.3281664012,\n",
       " 551896.0072724504,\n",
       " 333543.3782195783,\n",
       " 312110.4773245474,\n",
       " 261908.47349937842,\n",
       " 323892.1742231614,\n",
       " 339833.25901467865,\n",
       " 384780.0373902747,\n",
       " 283294.9396548247,\n",
       " 343943.58887032594,\n",
       " 257710.95941963437,\n",
       " 589813.7161657086,\n",
       " 660390.2384355494,\n",
       " 274942.0915096975,\n",
       " 739305.9189514222,\n",
       " 452883.3104312591,\n",
       " 433352.55427590787,\n",
       " 532112.8185393681,\n",
       " 461907.18665493996,\n",
       " 423198.96201026253,\n",
       " 827010.8455503191,\n",
       " 372726.5415120375,\n",
       " 459780.9623870027,\n",
       " 379771.2324166541,\n",
       " 351478.3865323059,\n",
       " 910085.1439474464,\n",
       " 617364.736682902,\n",
       " 512439.22015988297,\n",
       " 776118.8738778468,\n",
       " 923641.8712804004,\n",
       " 401589.31275899673,\n",
       " 260310.07885844872,\n",
       " 388434.3359482733,\n",
       " 482981.13977189315,\n",
       " 251093.37789741444,\n",
       " 411950.16526024864,\n",
       " 468673.8900402823,\n",
       " 573442.8880972876,\n",
       " 667767.8824629949,\n",
       " 535638.0504824913,\n",
       " 1115796.4275081255,\n",
       " 926585.1811608179,\n",
       " 839247.9838500997,\n",
       " 596243.2586207272,\n",
       " 655663.4258498545,\n",
       " 578533.1894214075,\n",
       " 485258.9092419751,\n",
       " 645013.0213329642,\n",
       " 371397.8472894969,\n",
       " 335111.5532865908,\n",
       " 353837.1287972849,\n",
       " 361322.29666965874,\n",
       " 339119.32271388645,\n",
       " 283506.2866344818,\n",
       " 312090.1511556535,\n",
       " 447979.53677905106,\n",
       " 463399.1125222817,\n",
       " 626407.1297453416,\n",
       " 392354.1727096395,\n",
       " 466576.6643765069,\n",
       " 587063.2005299993,\n",
       " 429160.1946570248,\n",
       " 417090.05952552136,\n",
       " 355555.2994472039,\n",
       " 678910.6167515508,\n",
       " 350548.36649334675,\n",
       " 257456.98150292278,\n",
       " 311608.54463009833,\n",
       " 480591.55103086605,\n",
       " 527034.9412899598,\n",
       " 678142.7520302385,\n",
       " 471630.0362879195,\n",
       " 468833.33756835957,\n",
       " 269319.1383628089,\n",
       " 432761.8271537436,\n",
       " 359521.8022817538,\n",
       " 348071.6614965009,\n",
       " 375874.9255039593,\n",
       " 655861.0675412854,\n",
       " 1592873.1593512115,\n",
       " 1286938.8185779802,\n",
       " 264878.333891826,\n",
       " 490720.8300356899,\n",
       " 492141.849838908,\n",
       " 1676652.3163282797,\n",
       " 456186.0496625068,\n",
       " 459894.6035396688,\n",
       " 324068.90065324435,\n",
       " 383493.9809945546,\n",
       " 518575.88814362994,\n",
       " 738112.4893648627,\n",
       " 783563.1244738499,\n",
       " 312202.589446576,\n",
       " 511099.3345524521,\n",
       " 308082.2210230545,\n",
       " 505210.9483342484,\n",
       " 1433176.904961733,\n",
       " 356684.3676063906,\n",
       " 421851.1307201459,\n",
       " 457116.5636751931,\n",
       " 356684.3676063906,\n",
       " 333641.3518905175,\n",
       " 695674.1435629346,\n",
       " 794659.900771213,\n",
       " 349425.6251942444,\n",
       " 364039.8967859107,\n",
       " 359617.4229031737,\n",
       " 1732222.7189726878,\n",
       " 535068.290283758,\n",
       " 497844.68657640985,\n",
       " 450727.03620652604,\n",
       " 525839.1820195254,\n",
       " 748827.5563974769,\n",
       " 342755.15078944573,\n",
       " 1274832.2364916215,\n",
       " 907797.0874624014,\n",
       " 461132.63992812147,\n",
       " 357995.718243948,\n",
       " 477350.9165230443,\n",
       " 702972.3071167909,\n",
       " 299363.16985314904,\n",
       " 347490.00301151566,\n",
       " 385583.225825998,\n",
       " 350301.1832494516,\n",
       " 361365.9003791864,\n",
       " 2377302.6550318887,\n",
       " 342462.0259394782,\n",
       " 442533.67869896407,\n",
       " 460505.17158045183,\n",
       " 573733.9817294024,\n",
       " 413740.4901119927,\n",
       " 470588.6270733826,\n",
       " 309586.4663394466,\n",
       " 518164.72683775116,\n",
       " 555129.1812488779,\n",
       " 704024.9223131307,\n",
       " 838563.6858607081,\n",
       " 531720.5080644668,\n",
       " 430895.1117498068,\n",
       " 722079.6862795965,\n",
       " 347114.5432524583,\n",
       " 348071.6614965009,\n",
       " 525289.4759730421,\n",
       " 512608.4079393049,\n",
       " 468031.4632262561,\n",
       " 907523.0315732388,\n",
       " 368670.9641848256,\n",
       " 3150543.5388692697,\n",
       " 631476.9292883149,\n",
       " 756276.3234747434,\n",
       " 1037490.6302645598,\n",
       " 505890.2646771728,\n",
       " 676258.9396934211,\n",
       " 931237.1376342436,\n",
       " 343743.1823687857,\n",
       " 717509.503911205,\n",
       " 435429.15719883516,\n",
       " 472028.8801813881,\n",
       " 342641.63800903375,\n",
       " 285736.71993617725,\n",
       " 440278.0994160663,\n",
       " 413766.2524762319,\n",
       " 1383601.462302291,\n",
       " 307446.32262016047,\n",
       " 343451.6997008027,\n",
       " 527565.1414302882,\n",
       " 360525.668990187,\n",
       " 318563.67657521495,\n",
       " 510932.08118246525,\n",
       " 367161.7698030749,\n",
       " 443945.04957423516,\n",
       " 480470.6117787987,\n",
       " 445451.6971689233,\n",
       " 369057.4584721144,\n",
       " 633875.1513848298,\n",
       " 356833.84820410784,\n",
       " 319433.401208838,\n",
       " 824002.2157794359,\n",
       " 455176.2045295816,\n",
       " 260196.92463155423,\n",
       " 349142.51580018597,\n",
       " 655861.0675412854,\n",
       " 654476.594549705,\n",
       " 496976.76669750956,\n",
       " 447768.8787003591,\n",
       " 450041.63284281193,\n",
       " 566044.5942892281,\n",
       " 473369.81878126867,\n",
       " 544379.8594678424,\n",
       " 328710.68664138264,\n",
       " 566873.007226201,\n",
       " 341267.9390000997,\n",
       " 811749.4325267588,\n",
       " 447979.53677905106,\n",
       " 383959.30227213894,\n",
       " 335360.0714266266,\n",
       " 326803.1792743221,\n",
       " 363241.3281029566,\n",
       " 295976.8341142817,\n",
       " 839032.2736819331,\n",
       " 1581339.4672457785,\n",
       " 957400.3419730539,\n",
       " 447607.85335476993,\n",
       " 805281.9588466173,\n",
       " 476650.50485923997,\n",
       " 799353.9554538282,\n",
       " 343475.24057761626,\n",
       " 398182.47963500355,\n",
       " 491241.4977416735,\n",
       " 264878.333891826,\n",
       " 297738.9345423346,\n",
       " 460478.39751330856,\n",
       " 463399.1125222817,\n",
       " 571384.2367673634,\n",
       " 299464.1172942443,\n",
       " 507864.2898549793,\n",
       " 257710.95941963437,\n",
       " 660878.9887965766,\n",
       " 302872.9375883332,\n",
       " 497926.26718615356,\n",
       " 302809.43959805823,\n",
       " 391533.17352977593,\n",
       " 485947.1306550893,\n",
       " 616354.894917968,\n",
       " 467863.92401900113,\n",
       " 928116.433433637,\n",
       " 260586.4185053957,\n",
       " 1743514.6988598607,\n",
       " 474019.85719727987,\n",
       " 438925.19362508674,\n",
       " 605146.4846841065,\n",
       " 675203.3311297316,\n",
       " 619570.4846603099,\n",
       " 341160.53382236016,\n",
       " 459662.4539782307,\n",
       " 475203.2627482161,\n",
       " 720351.9360593057,\n",
       " 279454.2162628617,\n",
       " 364354.4129804416,\n",
       " 472826.82537548087,\n",
       " 619026.9434757323,\n",
       " 339016.34399894136,\n",
       " 855051.4878354294,\n",
       " 555700.2883643581,\n",
       " 925426.2361397374,\n",
       " 994131.5869788385,\n",
       " 634010.1320945878,\n",
       " 369057.4584721144,\n",
       " 793682.5760675722,\n",
       " 356940.06491757836,\n",
       " 568603.9425364744,\n",
       " 683209.0021897355,\n",
       " 332599.5485141863,\n",
       " 786506.1610152104,\n",
       " 404746.8129639628,\n",
       " 993300.8451450893,\n",
       " 380597.43646750686,\n",
       " 504467.3440592176,\n",
       " 651685.7965273235,\n",
       " 575475.5276541709,\n",
       " 353660.9339415865,\n",
       " 543505.8280511084,\n",
       " 377261.27180864825,\n",
       " 337009.3760564466,\n",
       " 546020.8446724899,\n",
       " 393474.725189192,\n",
       " 419050.5705860851,\n",
       " 372521.9693398297,\n",
       " 721975.7132649301,\n",
       " 636423.9811941009,\n",
       " 437323.2079775931,\n",
       " 460192.1233392044,\n",
       " 458673.1926029023,\n",
       " 301770.8903427334,\n",
       " 491330.46781961696,\n",
       " 432592.3723077814,\n",
       " 463441.8132076145,\n",
       " 561792.7339155406,\n",
       " 393810.28229657345,\n",
       " 378326.76658505,\n",
       " 403776.1720445492,\n",
       " 1405039.4511827421,\n",
       " 380597.43646750686,\n",
       " 343690.3519606228,\n",
       " 1205688.6270169667,\n",
       " 673586.4458117418,\n",
       " 380922.0328559174,\n",
       " 433475.40229791484,\n",
       " 476195.52416350413,\n",
       " 656173.7430221088,\n",
       " 457315.45358095644,\n",
       " 395780.22167181794,\n",
       " 433443.9620772031,\n",
       " 463399.1125222817,\n",
       " 433352.55427590787,\n",
       " 479250.32169370446,\n",
       " 468446.8432810708,\n",
       " 350423.8754131625,\n",
       " 474283.62856703094,\n",
       " 613598.3645633971,\n",
       " 422607.74182191945,\n",
       " 269556.31140392716,\n",
       " 378435.5078159693,\n",
       " 460597.3511858758,\n",
       " 449940.8089337756,\n",
       " 1087645.1294446548,\n",
       " 457053.5189944858,\n",
       " 394785.8348543247,\n",
       " 562991.3181221023,\n",
       " 334525.4109106917,\n",
       " 666848.2155960797,\n",
       " 404315.52831777575,\n",
       " 469751.61311855307,\n",
       " 488934.73144099565,\n",
       " 465422.05701243447,\n",
       " 558113.2435403246,\n",
       " 335612.27305202454,\n",
       " 598023.8741137695,\n",
       " 517509.7213240005,\n",
       " 702421.3174073144,\n",
       " 537181.6112178408,\n",
       " 510933.9924933058,\n",
       " 480460.7623710732,\n",
       " 521360.1169458946,\n",
       " 381451.83164871536,\n",
       " 361653.18372160086,\n",
       " 344656.3240349087,\n",
       " 646960.813585799,\n",
       " 379559.65562106355,\n",
       " 362520.4466939436,\n",
       " 278052.4450592724,\n",
       " 368172.6017483367,\n",
       " 384464.00870117097,\n",
       " 827010.8455503191,\n",
       " 2481422.9140909906,\n",
       " 449401.22301203664,\n",
       " 1186013.5260111815,\n",
       " 504164.3465729283,\n",
       " 267730.6943548209,\n",
       " 483428.3461865772,\n",
       " 448787.0738667613,\n",
       " 426311.57750377886,\n",
       " 670018.2061722692,\n",
       " 384733.75375459506,\n",
       " 828635.4271526396,\n",
       " 373503.6998204793,\n",
       " 274689.49578245496,\n",
       " 462025.29410495545,\n",
       " 637842.9641146517,\n",
       " 975734.3988801738,\n",
       " 370659.44375870505,\n",
       " 728653.2374700449,\n",
       " 449811.9062757273,\n",
       " 355941.5475945061,\n",
       " 538103.1768460537,\n",
       " 971199.427400683,\n",
       " 445981.01379381045,\n",
       " 467863.92401900113,\n",
       " 350423.8754131625,\n",
       " 457474.314617507,\n",
       " 1212522.6089212473,\n",
       " 446883.29390307085,\n",
       " 499763.29299736477,\n",
       " 385496.2205497416,\n",
       " 463889.97134418145,\n",
       " 319620.5576773401,\n",
       " 456247.074497145,\n",
       " 303513.86480420246,\n",
       " 337656.6241002606,\n",
       " 344714.29682904755,\n",
       " 392888.2849913071,\n",
       " 451455.20683018415,\n",
       " 623754.5759339202,\n",
       " 517024.7915784072,\n",
       " 336842.0635657679,\n",
       " 762853.0907524637,\n",
       " 672767.0025104528,\n",
       " 727488.0038445828,\n",
       " 350438.4501112742,\n",
       " 429160.1946570248,\n",
       " 657980.9078239832,\n",
       " 1960436.0230203713,\n",
       " 488925.95435905375,\n",
       " 687127.3175767243,\n",
       " 367731.27747116965,\n",
       " 366518.3034141244,\n",
       " 338112.5423923985,\n",
       " 682471.6665055754,\n",
       " 478102.82408717426,\n",
       " 317179.80521185894,\n",
       " 518806.8420605587,\n",
       " 345913.0691362353,\n",
       " 553911.9087800849,\n",
       " 797991.2275938315,\n",
       " 1053769.1206355738,\n",
       " 331782.01645355695,\n",
       " 536797.2079121631,\n",
       " 1202450.4507569387,\n",
       " 463399.1125222817,\n",
       " 804940.8871983739,\n",
       " 627106.0151455202,\n",
       " 459780.9623870027,\n",
       " 848278.149764492,\n",
       " 720318.7590747129,\n",
       " 255289.59977226908,\n",
       " 373348.1622445744,\n",
       " 279791.27636686864,\n",
       " 432768.2661649351,\n",
       " 538686.2164880685,\n",
       " 335522.1967260754,\n",
       " 487046.9929599978,\n",
       " 536714.7356173504,\n",
       " 411978.1301892339,\n",
       " 497727.59061759274,\n",
       " 353577.1339777553,\n",
       " 815019.2629030447,\n",
       " 343270.97042384796,\n",
       " 374240.0290259738,\n",
       " 375414.3200633079,\n",
       " 1174198.2023696736,\n",
       " 460013.46389019507,\n",
       " 284550.12717031967,\n",
       " 280369.45960092207,\n",
       " 472146.58139812405,\n",
       " 813191.070748863,\n",
       " 474449.3847479558,\n",
       " 670400.1460050668,\n",
       " 543372.8313961027,\n",
       " 448378.91853648133,\n",
       " 998613.0604021708,\n",
       " 275870.7134723314,\n",
       " 935951.8413696971,\n",
       " 637842.9641146517,\n",
       " 508725.2052712992,\n",
       " 1028900.8707003912,\n",
       " 482573.39421380154,\n",
       " 280026.5083864507,\n",
       " 876989.5732833497,\n",
       " 334218.58843912015,\n",
       " 631423.3662242404,\n",
       " 1414329.6452229351,\n",
       " 339729.3824159014,\n",
       " 503695.98514081445,\n",
       " 448244.81288747286,\n",
       " 369966.7511163681,\n",
       " 255348.89326017495,\n",
       " 865956.1067392726,\n",
       " 439960.75983607164,\n",
       " 284124.42360773135,\n",
       " 468896.1893811796,\n",
       " 357640.2927014486,\n",
       " 283702.19322117395,\n",
       " 496488.0073937055,\n",
       " 357488.1439406169,\n",
       " 418027.4836349586,\n",
       " 388081.74261796504,\n",
       " 481521.7037305273,\n",
       " 481320.07400951703,\n",
       " 537174.5231819393,\n",
       " 779014.3184633701,\n",
       " 335491.5574122303,\n",
       " 327446.51179400255,\n",
       " 379559.65562106355,\n",
       " 383493.9809945546,\n",
       " 327308.185975856,\n",
       " 404252.9623830129,\n",
       " 777329.2241703758,\n",
       " 501272.4259624582,\n",
       " 852594.6154180858,\n",
       " 825954.4018948479,\n",
       " 476701.397795427,\n",
       " 441240.40899045405,\n",
       " 481486.71871700324,\n",
       " 316412.90122833976,\n",
       " 404769.83081719663,\n",
       " 435687.054100738,\n",
       " 314432.5272392209,\n",
       " 471017.66896292043,\n",
       " 670400.1460050668,\n",
       " 417530.23265097226,\n",
       " 421775.4494279865,\n",
       " 459266.34717605927,\n",
       " 411391.9067919718,\n",
       " 1002918.0008313954,\n",
       " 817085.7134414525,\n",
       " 753449.0495421493,\n",
       " 371166.0284996994,\n",
       " 458673.1926029023,\n",
       " 399483.7156792691,\n",
       " 567720.8970394073,\n",
       " 265674.6922535054,\n",
       " 387344.9525828482,\n",
       " 351633.19459921104,\n",
       " 316677.186133274,\n",
       " 356929.7035282518,\n",
       " 281239.8571398335,\n",
       " 413722.20340679993,\n",
       " 588385.1627123079,\n",
       " 311329.7842380272,\n",
       " 442617.53786236263,\n",
       " 516743.3355236646,\n",
       " 492996.26627891714,\n",
       " 346486.9120248811,\n",
       " 337507.65124175075,\n",
       " 280871.12696515286,\n",
       " 534411.9143256469,\n",
       " 469418.82608239417,\n",
       " 1724134.5234806612,\n",
       " 459780.9623870027,\n",
       " 2164024.6090396475,\n",
       " 669656.8760385297,\n",
       " 1068263.2716069322,\n",
       " 621921.5239542047,\n",
       " 418196.7132680246,\n",
       " 458114.07917381346,\n",
       " 978304.1654668192,\n",
       " 685889.1074226965,\n",
       " 473386.8054663983,\n",
       " 473536.9440541739,\n",
       " 762014.5179332147,\n",
       " 495543.7257388574,\n",
       " 380090.14707958547,\n",
       " 400316.8777901297,\n",
       " 362186.37443234224,\n",
       " 468833.33756835957,\n",
       " 336932.30123443913,\n",
       " 736403.2718781068,\n",
       " 505899.71954516775,\n",
       " 793682.5760675722,\n",
       " 704791.3620240297,\n",
       " 508168.6144428037,\n",
       " 449352.93608276465,\n",
       " 479579.00524656253,\n",
       " 497191.0933448292,\n",
       " 501954.9828074352,\n",
       " 552968.8293241356,\n",
       " 308784.8922258069,\n",
       " 1060027.847520709,\n",
       " 311412.41654555715,\n",
       " 566385.4286172963,\n",
       " 274775.4346556806,\n",
       " 1729685.7064918005,\n",
       " 846706.1271372472,\n",
       " 623669.8064026579,\n",
       " 507059.2028750114,\n",
       " 639230.2586794645,\n",
       " 560276.8786702233,\n",
       " 347055.17294014804,\n",
       " 532640.2820907581,\n",
       " 464815.1879619447,\n",
       " 1004757.2326671112,\n",
       " 374479.9325889595,\n",
       " 498840.80218709045,\n",
       " 972938.7189976996,\n",
       " 536125.3262383564,\n",
       " 456892.6852488482,\n",
       " 448353.67465947487,\n",
       " 1073548.52560815,\n",
       " 319412.4295720018,\n",
       " 633917.6952034645,\n",
       " 781396.0473352784,\n",
       " 319412.4295720018,\n",
       " 627042.6721793409,\n",
       " 588963.1859594653,\n",
       " 396272.9706360912,\n",
       " 467186.80994739215,\n",
       " 614191.5861092773,\n",
       " 437995.97804125195,\n",
       " 350438.4501112742,\n",
       " 539153.6640510081,\n",
       " 274775.4346556806,\n",
       " 341204.7087728419,\n",
       " 469724.8631601205,\n",
       " 527609.4014416389,\n",
       " 449060.11993214826,\n",
       " 789232.5333765071,\n",
       " 425609.4023960667,\n",
       " 299978.17566669226,\n",
       " 325980.2565560939,\n",
       " 468446.8432810708,\n",
       " 731103.1209659193,\n",
       " 752230.2553060608,\n",
       " 368670.9641848256,\n",
       " 300322.42763782863,\n",
       " 383078.86385049194,\n",
       " 274828.89603248297,\n",
       " 831699.4685071241,\n",
       " 397593.9502382038,\n",
       " 283563.4564092842,\n",
       " 338438.70678932033,\n",
       " 488291.43822255684,\n",
       " 490950.72613215435,\n",
       " 468446.8432810708,\n",
       " 1135059.609165909,\n",
       " 1005323.6487897558,\n",
       " 1178066.1172215561,\n",
       " 433282.30414398416,\n",
       " 573741.5862586134,\n",
       " 369709.4132226816,\n",
       " 515564.4635569759,\n",
       " 354488.1187336898,\n",
       " 392318.0664427596,\n",
       " 347319.60302244464,\n",
       " 813538.7948339015,\n",
       " 463757.95724343666,\n",
       " 360693.8112267378,\n",
       " 402758.4270025493,\n",
       " 293557.31165456574,\n",
       " 531871.8553218042,\n",
       " 480649.8270809141,\n",
       " 426126.92478108685,\n",
       " 527910.794639736,\n",
       " 276564.850606677,\n",
       " 634183.0419835644,\n",
       " 523074.8272634734,\n",
       " 453915.066137477,\n",
       " 473369.81878126867,\n",
       " 299865.63381175173,\n",
       " 357130.13477511663,\n",
       " 1180139.7714640247,\n",
       " 761466.0316764516,\n",
       " 441543.3308061481,\n",
       " 320960.2710490468,\n",
       " 333750.4268695617,\n",
       " 1025031.4925152004,\n",
       " 548279.761518737,\n",
       " 436387.7712711664,\n",
       " 275084.1021251388,\n",
       " 384751.08517653524,\n",
       " 298700.7282575576,\n",
       " 843655.2629766768,\n",
       " 255965.9519361822,\n",
       " 323705.6389697588,\n",
       " 298056.17684567487,\n",
       " 470588.6270733826,\n",
       " 480662.52730676,\n",
       " 590436.7061755608,\n",
       " 576292.8615527266,\n",
       " 395044.4806244125,\n",
       " 470257.28094847855,\n",
       " 561732.831458627,\n",
       " 488750.1865125038,\n",
       " 472054.6357057226,\n",
       " 617364.736682902,\n",
       " 506719.6099618461,\n",
       " 784000.9519965434,\n",
       " 392955.88584504754,\n",
       " 317065.35724786215,\n",
       " 467407.9277201699,\n",
       " 585238.6395762927,\n",
       " 468262.8689116955,\n",
       " 490281.06313323823,\n",
       " 941990.7928724391,\n",
       " 532349.6408135114,\n",
       " 741116.4721773332,\n",
       " 618957.1592129376,\n",
       " 803923.9889245467,\n",
       " 274949.479120978,\n",
       " 477657.2999713924,\n",
       " 466883.4057461512,\n",
       " 833383.883775811,\n",
       " 880280.1855323592,\n",
       " 278402.3129874008,\n",
       " 260310.07885844872,\n",
       " 494943.1957708172,\n",
       " 298167.5855681265,\n",
       " 701145.2241048259,\n",
       " 388865.2198831246,\n",
       " 478245.9301767312,\n",
       " 518010.67924287065,\n",
       " 387663.1225620359,\n",
       " 298056.17684567487,\n",
       " 293650.5660031007,\n",
       " 323560.2270055239,\n",
       " 633518.5987825533,\n",
       " 280349.39511956903,\n",
       " 373346.0215865177,\n",
       " 476770.69625931466,\n",
       " 387948.62263317604,\n",
       " 521360.1169458946,\n",
       " 371806.17145059176,\n",
       " 392318.0664427596,\n",
       " 463757.95724343666,\n",
       " 353932.93086204503,\n",
       " 1189226.9464988452,\n",
       " 627333.628478755,\n",
       " 328479.879156657,\n",
       " 457171.7621781201,\n",
       " 777360.2403859579,\n",
       " 633235.5567377652,\n",
       " 270798.6363849707,\n",
       " 469055.5471975446,\n",
       " 391533.17352977593,\n",
       " 299504.14608487836,\n",
       " 876731.9724234802,\n",
       " 475230.3897330188,\n",
       " 552888.1377299641,\n",
       " 496498.12204579136,\n",
       " 681341.1359833303,\n",
       " 571341.7202033615,\n",
       " 827507.4477432035,\n",
       " 663883.3839898735,\n",
       " 790744.6932837889,\n",
       " 374298.40864434175,\n",
       " 1031212.9759544872,\n",
       " 617728.3507861847,\n",
       " 781215.8480843862,\n",
       " 453775.4957249096,\n",
       " 465918.0578154819,\n",
       " 384733.75375459506,\n",
       " 463757.95724343666,\n",
       " 373348.1622445744,\n",
       " 776118.8738778468,\n",
       " 518751.27621310466,\n",
       " 490861.828710161,\n",
       " 280086.9984953593,\n",
       " 274775.4346556806,\n",
       " 299491.5422991887,\n",
       " 807394.2062827045,\n",
       " 654590.5729410781,\n",
       " 458558.02610783436,\n",
       " 360755.29045466665,\n",
       " 466290.1894064732,\n",
       " 363241.3281029566,\n",
       " 654678.7054883533,\n",
       " 274843.84983665263,\n",
       " 652004.5488037181,\n",
       " 527366.7698258385,\n",
       " 355817.2864963232,\n",
       " 277738.5296745239,\n",
       " 487385.7528715705,\n",
       " 499589.250182174,\n",
       " 377203.7932646029,\n",
       " 689071.4384075112,\n",
       " 337656.6241002606,\n",
       " 388865.2198831246,\n",
       " 536005.7487713654,\n",
       " 302574.25061671104,\n",
       " 562884.9349121407,\n",
       " 688917.0813207375,\n",
       " 711801.5913705168,\n",
       " 484788.93592149473,\n",
       " 427116.92613186064,\n",
       " 705682.820320224,\n",
       " 513237.8027150617,\n",
       " 654590.5729410781,\n",
       " 1562632.2904588007,\n",
       " 544512.6453292847,\n",
       " 338649.101557291,\n",
       " 692982.5865799004,\n",
       " 856344.7737992728,\n",
       " 340065.5800859708,\n",
       " 419500.9232168953,\n",
       " 373803.2192843129,\n",
       " 289207.98998556414,\n",
       " 510892.12532937666,\n",
       " 502602.90306634986,\n",
       " 403851.19924754737,\n",
       " 488164.631051262,\n",
       " 329099.3746293139,\n",
       " 372181.2686880648,\n",
       " 256506.22575170145,\n",
       " 351478.3865323059,\n",
       " 343690.3519606228,\n",
       " 331790.7144288234,\n",
       " 343034.535019898,\n",
       " 333250.1813885366,\n",
       " 844315.3154056699,\n",
       " 425113.1128215326,\n",
       " 437047.086585454,\n",
       " 713201.302132383,\n",
       " 356456.1394833817,\n",
       " 255965.9519361822,\n",
       " 370311.99417052575,\n",
       " 296725.0826409486,\n",
       " 583415.1788481643,\n",
       " 301542.7571223729,\n",
       " 517639.7708971669,\n",
       " 803007.9755779018,\n",
       " 586151.3127742549,\n",
       " 559091.4072602082,\n",
       " 500271.9986905918,\n",
       " 267376.2718053758,\n",
       " 932579.7897287648,\n",
       " 635886.4027799169,\n",
       " 838563.6858607081,\n",
       " 328716.15498589753,\n",
       " 462261.44819840026,\n",
       " 797821.4729477076,\n",
       " 510025.22294741945,\n",
       " 258593.6921656148,\n",
       " 1161835.1868071333,\n",
       " 311608.54463009833,\n",
       " 636110.1637493577,\n",
       " 468896.1893811796,\n",
       " 789097.7255115305,\n",
       " 550309.8299249524,\n",
       " 449811.9062757273,\n",
       " 458673.1926029023,\n",
       " 443108.95608591614,\n",
       " 554549.4717026844,\n",
       " 1256957.5260796088,\n",
       " 486035.8504244507,\n",
       " 861548.1958176333,\n",
       " 483705.00713816704,\n",
       " 378595.90948133235,\n",
       " 274333.2804682418,\n",
       " 437176.8733995883,\n",
       " 539037.2207159458,\n",
       " 484813.6693569217,\n",
       " 456332.74361342686,\n",
       " 298700.7282575576,\n",
       " 361647.76123162254,\n",
       " 353525.3428706987,\n",
       " 511589.11568672874,\n",
       " 561507.7367021821,\n",
       " 280246.8003892636,\n",
       " 386273.7079332293,\n",
       " 498831.0180595373,\n",
       " 627042.6721793409,\n",
       " 987802.1169961076,\n",
       " 662996.374868023,\n",
       " 360222.38571294316,\n",
       " 784310.5259604265,\n",
       " 714850.5377565615,\n",
       " 838572.4381257284,\n",
       " 352221.40953500516,\n",
       " 2322861.5062120114,\n",
       " 368282.39959410543,\n",
       " 377311.26180032967,\n",
       " 503556.5204620676,\n",
       " 334296.99275209813,\n",
       " 352824.3519629203,\n",
       " 551424.0584545814,\n",
       " 661446.3827946872,\n",
       " 729008.5787895735,\n",
       " 274698.12405119586,\n",
       " 353660.9339415865,\n",
       " 547115.0368813077,\n",
       " 2264312.6445991476,\n",
       " 553365.0495175107,\n",
       " 466725.1383065857,\n",
       " 924311.7928201195,\n",
       " 623342.9660738359,\n",
       " 560922.4189823478,\n",
       " 847634.3145052136,\n",
       " 717026.4867252677,\n",
       " 449811.9062757273,\n",
       " 296429.5552154263,\n",
       " 430041.13459197216,\n",
       " 651200.2978682101,\n",
       " 552367.0981850999,\n",
       " 281955.7085526171,\n",
       " 299504.14608487836,\n",
       " 578449.1405425437,\n",
       " 412101.60493539716,\n",
       " 666734.5158893132,\n",
       " 486792.9989346505,\n",
       " 920557.5486526856,\n",
       " 384288.6212815032,\n",
       " 296653.528865826,\n",
       " 389205.89042305935,\n",
       " 286731.25723841187,\n",
       " 356684.3676063906,\n",
       " 340065.5800859708,\n",
       " 405100.6962449867,\n",
       " 885659.0809274476,\n",
       " 574088.3805340312,\n",
       " 363400.26992533694,\n",
       " 501843.82680088,\n",
       " 665707.3780364452,\n",
       " 258511.76313412795,\n",
       " 530539.0748010859,\n",
       " 353360.5060823347,\n",
       " 641589.5642537579,\n",
       " 640334.8938891413,\n",
       " 462804.7976476915,\n",
       " 287279.87503488344,\n",
       " 435219.8000733464,\n",
       " 352241.61574400484,\n",
       " 875190.1709199346,\n",
       " 359611.53963652265,\n",
       " 574205.4493123688,\n",
       " 413800.47691993706,\n",
       " 385496.2205497416,\n",
       " 344741.5471691124,\n",
       " 343144.07886420935,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
